\section{First-order prolog}
\label{sec:basic-elpi}

\subsection{First-order interpreter}
In this first section we start with a first-order version of a logic language.

The interpreter we are going to present is a system based on the derivation
rules depicted in \cref{fig:basic-interp}. Its structure is similar to the
operational semantics in \cite{1990Vink}. The choice to use operational
semantics, rather than denotational semantics (as in \cite{2011king}), is due to
our preference for maintaining a concrete representation of the objects we are
manipulating. Instead of using continuations to store the state of a choice
point, we prefer to represent this information as lists containing the
alternatives, along with the substitution existing at the moment the choice
point was created.

\begin{figure}
  
  \ruleStopM{.45}
  \ruleFailM{.45}
  \vspace{0.3em}%
  
  \ruleUnifM{.50}
  \ruleBangM{.40}
  \vspace{0.3em}%
  
  \ruleCallM{1}
  
  \caption{Basic derivation rules}
  \label{fig:basic-interp}
\end{figure}

The semantics expressed by \runCmd{g}{a}{s}{a'}{s'} relates a list of
goals $g$, a list of alternatives $a$ and an initial substitution $s$ with a
new substitution $s'$ and a new list of alternatives $a'$. The list of goals
should be understood as a list of goal put in cunjunction, whereas the
alternatives represent a disjunction of goals. In the derivation above we use
the two colon symbol ($::$) has separator between the head of a list and its tail;
$@$ stands for list concatenation; $[\ ]$ is the empty list. $\prog p$ is the
application of \prog to the predicate $p$; it returns a list of clauses. The
function $\mathcal{F}$ is defined as follows:
%
$$
\mathcal{F}(\prog, p, i, o, s, a) := 
  [(s, (
      \Cons{(\prog, i =_i i', a)}
        \Cons{(\prog, o =_o o', a)}
          {[(\prog, g, a) \mid g \in bs]})) \mid \clauseCmd{p}{i'\ o'}{bs} \in \prog\ p]
$$

It takes a list of clauses, two terms, i.e. $i$ and $o$ representing the two
arguments of the predicate call a substitution $s$ and a list of alternatives $a$. For each clause
$(cl\ i'\ o'\ bs)$, it builds a new list of pairs where the first argument
is the substitution $s$ and the second are goals made by 1) the unification of
the first argument $i'$ with the first argument $i$ of the predicate call 2) the
unification between the second arguments $o'$ and $o$ 3) the map of each premise
$g$ in the body $bs$ which return a new list of goals having the program $p$,
the atom $g$ and the list of alternatives $a$.

The behaviour of the program depends on the shape of the list of goal and its
alternatives. Starting from a configuration it is possible to execute the
program. It means that it exists a sequence of reductions allowing to reach the
\textit{stop rule}, symbolyzed with \ruleStop.

More precisely, \textit{stop rule} caputres the
configurations with an empty list of goals. By convention, this configuration is
the final one: there are no more goal to treat; therefore, we can stop and
return the same alternatives and substitution received in input.

The \textit{fail rule} (\ruleFail) is used to consume the list of alternatives,
if for example, the current goal leads to a failure or a loop. In fact, this
rule allows to break loop since it can non-determinalistically applied on any
configuration, provided that the list of alternatives is not empty. It is
possible to make the algorithm deterministic by chainging \ruleFail so that it
is applied if the current goal is a call to a predicate with no alternatives,
but we prefer to simplify our rule system.

The \textit{unify rule} (\ruleUnif) is applied when a unification $t1 = t2$
occurs in the head of the current goal list. It calls the unification algorithm
between the two terms $t1$ and $t2$ under the current substitution $s$ and
returns the updated substitution $s'$. Finally the \run procedure is called the
list of remaining goals $gs$ and the new substitution $s'$.

The \textit{cut rule} (\ruleBang) refers to goals where the first conjunct is is
the \texttt{!} atom. This is where the cut-alternatives $ca$ become interesting.
The rule tells that the list of goal $gl$ can be launched using $ca$ and the
new list of alternatives.

The \textit{call rule} (\ruleCall) deals with goals starting with a predicate
call. In this case the function $\mathcal{F}$ is called with the parameters as
explained above and if the result of this operation is the list
\ConsHd{b}\ConsTl{bs}, then a recursive call to \run is done by prepending $b$ to
the list of remaining goals $gl$ and $bs$ is prepended to the list of
alternatives $a$.

The combination of the \ruleCall and \ruleBang are crucial to reproduce the
behaviour of the hard cut. When a call to a predicate gives several rules as new
choice points, thanks to the function $\mathcal{F}$ we can associate each atom
of each body of $\prog p$ to the current alternatives $a$. If a clause $b_i$ in
\ConsHd{b}\ConsTl{bs} with $0 \leq i \leq | bs |$ has a cut, then all the
choice points $b_j$ with $i < j \leq | bs |$ will be cut away.

As an example, let's consider the following program called \prog:

\begin{elpicode}
p1 X Y :- p2 X Y.           % r1
p1 3 3.                     % r2
p2 X Y :- p3 X Y, !, Y = 1. % r3
p2 1 1.                     % r4
p3 1 2.                     % r5
\end{elpicode}

{
\def\goalG{\goalCmd{\prog}{call\ (\text{\elpiIn{p1 3 Z}})}{\EmptyList}}

Let $\mathcal{G} :=\ \goalG$ be a goal, the execution of
``\runCmd{\mathcal{G}}{\EmptyList}{\EmptySubst}{?A}{?\subst}'' will apply \ruleCall
producing a new goal for the rule $r1$ and an alternative list containing $r2$,
we leave out details concerning unifications of head terms. The execution of
$r1$ will try to solve $r3$. This will add three goals \elpiIn{r 1 Y, !, Y = 1}
with cut-alternatives equal to $r2$ whereas the new alternatives will be $r4 @
r2$. The execution of $r3$ will solve \elpiIn{p3 1 Y} with substitution $s :=
\{X \gets 1; Y \gets 2\}$. The \elpiIn{!} will cut away the alternative $r4 @
r2$ and will keep the cut-alternative $r2$. The failing premise \elpiIn{Y = 1}
will cause a backtracking thanks to \ruleFail and try to apply $r2$ from the
empty substitution. This last unification succeed with final substution $?\subst :=
\{X \gets 3; Y \gets 3\}$ and final list of alternatives $?A := \EmptyList$.

% \begin{myRule}{1}
%   \AxiomC{}
%   \RightLabelM{\ruleCall}
%   \UnaryInfC{\runCmd{\mathcal{G}}{\EmptyList}{\EmptySubst}{?A}{?S}}
% \end{myRule}

}


\subsection{Modes in the first-order setting}

% Mode checking in this setting is performed using the classical groundness
% verification where term groundness is derived from their flow in the body of a
% clause. We start with the hypothesis that a term in input position in the head
% of a clause is ground. The premises in the body representing calls to predicates
% must be called with ground input terms, while output are supposed to become
% ground. At the end of the body analyse, we check that the output terms in the
% head of the clause has become ground. If this is not the case, or if a call in
% the body is done with a non-ground term in input position a mode-checking error
% is raised.

The modes we authorize in this setting are two, namely the input and the output
modes. Since we want our predicates the be called with non-ground input terms,
the meaning we attach to our modes is quite particuar. In this setting, the user
is free to pass any term (even variables) in input position. Therefore, no real
static check is performed on the clauses of the database: in this stage of our
first-order version of the langauge, we give the user high freedom on the shape
that its predicate call may have.

Our interpretation of mode follows the idea of the read and write modes\todo{usare la parola match}
explained in \cite{1991ait-wam}. Essentially, we consider an input argument as
an object received in read mode in the head of the clause. For example, if the
entry term is a variable and the corresponding term in the head of the clause is
a rigid term, the unification will fail. This is because, the \unify procedure
of the interpreter cannot write in input head-arguments. On the other hand, it
is always possible to unify a non-flex input term with a variable if this
variable appears in the head of the clause. Finally no constraint is put on
output terms, which can be read and written at any time.

Formally, let $\texttt{unifyc}\ t_1\ t_2 \to \subst$ be the standard unification
relation between two terms $t_1$ and $t_2$ returning the most general unifier
$\subst$ if it exists; let $\texttt{vars}\ t$ be a function returning
the list of variables name appearing in the term $t$; let $p$ be a predicate of
arity $N$, $p\ \vec{x}$ a call to that predicate and $p\ \vec{y}$ the head of a
clause implementing $p$. Then for any index $i$, $0 \leq i < N$, such that $i$
is an input argument of $p$, then, if $\texttt{unifyc}\ x_i\ y_i \to \subst$,
then no variable in $\texttt{vars}\ x_i$ has been assigned in $\subst$.

As a mean of example, consider following program.

\begin{elpicode}
  pred p i:int, o:int.
  p 1 1.          % p1
  p X 2 :- X = 7. % pX
\end{elpicode}


The predicate \elpiIn{p} is decorated with a type\&mode signature. It tells not
only that the first two arguments should be of type \elpiIn{int}, but also that
the first argument is an input (\modeAlone{i}) and the second an output
(\modeAlone{o}). This means that the query \elpiIn{p Y Z} succeed exactly once:
the rule \elpiIn{p1} could not be applied, since the variable \elpiIn{Y} is in
input (i.e. read mode) and cannot be instantiated to $1$ in the head of the
clause. Note, that the application of \elpiIn{pX} on the query succeed and
producing the substitution $\subst = \{Y \mapsto 7, Z \mapsto 2\}$: \elpiIn{Y}
can be instantiated in the body of a clause. Moreover, the query
\elpiIn{p 1 Z} has one solution with $\subst = \{Z \mapsto 1\}$ due to the
application of \elpiIn{p1}. Note that the rule \elpiIn{pX} is a valid choice
point for the query: the head unifies with the query assigning the local
$\exists$variable \elpiIn{X} to $1$, but a failure will occur in the body
while unifying the value of \elpiIn{X} and \elpiIn{7}.

Moreover, \elpi provides a way to further control the unification on input
arguments. The user can put the \uvar keyword in front of a variable name in a
input argument. This tells the unification engine to unify this head term with
only flexible term. For example, we can add the rule ``\elpiIn{pUX} := \elpiIn{p
(uvar X) 2 :- X = 7}'' to the previous database. The \uvar keyword does not
allow the call \elpiIn{p 1 Z} to be unified with \elpiIn{pUX} since \elpiIn{1}
is not a variable.

We think that the combination of the input mode and the \uvar keyword give
the user a powerful tool to  control how unification is dynamically performed
at runtime.

\subsection{First-order determinacy checker}

Mode checking is an essential ingredient from which determinacy checking can be
performed. Before explaining the interaction between these two, we need to take
some time to talk about mutual clauses exclusiveness.
%
% \paragraph{Mutual exclusive clauses}
From \cite{1989Warren}, we know that at most one clause can be applied
successfully for any determinate predicate.

In order to satisfy this (necessary by not sufficient) condition, we need 
that all the clauses of a determinate predicate $p$ are mutually exclusive.
Mutual exclusiveness can be stated as follows:

\begin{definition}[Non-overlapping clauses]
  For any pair of clauses of the same predicate ``\clauseCmd{p}{\vec{x}}{b1}''
  and ``\clauseCmd{p}{\vec{y}}{b2}'', we say that they are non-overlapping if
  there exists an input position $i$ such that $\forall \sigma, \sigma\ x_i \neq
  \sigma\ y_i$
  \label{def:mut-excl}
\end{definition}

In \cite{1996henderson}, mode checking (with groundness check)
ensures that at most one clause can be applied to
any predicate such that its clauses respect \cref{def:mut-excl}. 
In fact, thanks to mode checking,
any call is validated if its terms in input position are ground.
Groundness, thanks to \cref{def:mut-excl}, ensures that it exists at most one clause 
which can be executed on that call.\\
Note also that the groundness check ensures that outputs become ground. This is
a fundamental property; otherwise, outputs would be pointless. If an output does
not become ground, it cannot serve as the input for another predicate call.
Otherwise, two clauses could potentially be applied to the same predicate call.


In our setting, we slightly extend this definition so that the \elpi's \uvar
keyword is taken into account.

\begin{definition}[Mutual exclusiveness with \uvar]
  A clause with an input term marked with the \elpiIn{uvar} keyword,
  does not overlap with any other rigid-head term.
  \label{def:mut-excl-uvar}
\end{definition}

This means that a term marked with the \uvar keyword in the head of a clause
overlaps only with unification variables or with another term marked with
\uvar.

\begin{theorem}
  The \elpi input/ouput modes and the (hard-)cut operator guarantee that for any
  predicate $p$ whose clauses respect
  \cref{def:mut-excl,def:mut-excl-uvar}, there exists at most
  one succeeding clause for any call to $p$.
\end{theorem}

\begin{proof}
  Without loss of generality, we take a program \prog with only binary
  predicates representing respectively an input and an output. Let $p$ be a
  predicate in \prog such that all clauses respect
  \cref{def:mut-excl,def:mut-excl-uvar}. Let
  ``$p\ t_1\ t_2$'' be a valid call for $p$. Let ``$c_1 :=
  \clauseCmd{p}{t_1'\ t_2'}{b_1}$'' and ``$c_2 := \clauseCmd{p}{t_1''\
  t_2''}{b_2}$'' be two clauses implementing $p$.  We need to prove that the head
  of $c_1$ and $c_2$ cannot unify both with the call. Note that the absence of
  groundness check avoid us from saying that $t_1$, which is the input of the
  call, is a ground term. We reason by induction on the shape of $t_1$ and show
  that it cannot unify simultaneously with $t_1'$ and $t_1''$, i.e. at most one
  between $c_1$ and $c_2$ can be applied on the goal.
  \begin{itemize}
    \item Case 1: $t_1$ is a constant. A constant, in input position, matches with
          the same constant of a variable. By the definition of our unification
          of input terms, $t_1$ only matches with a input terms which is either
          the same constant or a variable (but not a term marked with \uvar).
          Due to \cref{def:mut-excl}, $t_1'$ and $t_1''$ cannot be neither the
          constant $t_1$ nor a unification variable nor a combination of the
          two. Therefore $c_1$ and $c_2$ cannot be applied both of the call to
          $p$.
    \item Case 2: $t_1$ is a variable. A variable, in input position, matches
          with another variable or a term marked with \uvar (but not a rigid
          term). By \cref{def:mut-excl}, $t_1'$ and $t_1''$ cannot be both
          unification variables and, by \cref{def:mut-excl-uvar}, they cannot be
          a combination of a unification variable and a term marked with the
          \uvar keyword. This means that at most one of the two clauses 
          can be applied on the call to $p$.
    \item Case 3: $t_1$ is a compond term: a term starting with rigid head with
          potentially flexible subterms. If the heads of $t_1'$ and $t_1''$ have
          the same head as $t_1$ then the unification of $t_1$ proceed on the
          subterms, but, by induction hypothesis, only one between $t_1'$ and
          $t_1''$ can unify with $t_1$. If the heads of $t_1'$ and $t_1''$ are
          different then only we are sure that at most one of the two clause
          can be applied on the call.
  \end{itemize}
\end{proof}


As explained in \cite{1989Warren}, thanks to the (hard-)cut operator, mutual
exclusiveness can be relaxed.

\begin{definition}[Mutual exclusiveness with \cut]
  Two clauses for the same predicate are mutually exclusive if the
  chronological antecedent has a cut in its body.
  \label{def:mut-excl-cut}
\end{definition}

% This ensures that if we reach the cut
% operator in the first clause, the second clause is not considered as a choice
% point. Conversely, if one of the premises before the cut fails, then the second
% clause will be tried. In both situations, the two clauses cannot be applied
% simultaneously to the same predicate call.

This definition allows overlapping clauses to
exist in a database under the condition that the antecedent has a cut
guaranteeing that at most one clause can be applied on a predicate call. 

% \paragraph{Determinate clauses after last \cut}
The second, but no less important, condition for a predicate to be determinate
is the following:

\begin{definition}[Determinate premises after last \cut]
  In each clause of a determinate predicate, the premises after the last \cut
  operator are only calls to determinate predicates. 
  \label{def:det-prem-cut}
\end{definition}

This guarantees that any output produced is uniquely determined, i.e. no two
solutions can be produced on the same call.

Thanks to this last definition, we can give our definition of determinacy
checking:

\begin{definition}[Determinacy checking]
  Determinacy checking is the combination of mutual-exclusive check up to
  \cref{def:mut-excl,def:mut-excl-uvar,def:mut-excl-cut} and
  \cref{def:det-prem-cut}.
  \label{def:det-check}
\end{definition}

Before continuing our discussion about determinacy, we prove an auxiliary
lemma.
% {goal[]}{alts[]}{subst}{alts_res}{subst_res}
\begin{lemma}
  For each determinate predicate $p$,
  it always exists an implementation so that each clause of $p$ have at least
  one \cut in its body without loosing in expressiveness.
  % Forall list of goals $gs$ and list of alternatives $alts$ and $a$ such that
  % they share a program \prog, $\forall \subst\ \subst',
  % \runCmd{gs}{alts}{\subst}{a}{\subst'} \to \exists \prog',
  % \runCmd{gs'}{alts'}{\subst}{a'}{\subst'}$ such that all clauses of each
  % determinate predicate in $\prog'$ has a \cut and $gs', alts'$ and $a'$ are
  % list of goals and alternatives build from resp. $gs, alts$ and $a$ where the
  % \prog has been by $\prog'$.
  \label{th:all-cut}
\end{lemma}

\begin{lemma}
  Forall list of goals $gs$ and list of alternatives $alts$ and $a$ such that
  they share a program \prog,
  $$
  \begin{array}{l}
  \forall \subst\ \subst' i\ o\ p, \mathtt{pred\_functional}(p)\\
  \runCmd{[\goalCmd{\prog}{\callCmd{p}{i}{o}}{a}]}{[]}{\subst}{a'}{\subst'} \iff \\
  \runCmd{[\goalCmd{\mathtt{tailcut}(\prog(p))}{\callCmd{p}{i}{o}}{a}]}{[]}{\subst}{a'}{\subst'}
  \end{array}
  $$
  
  such that all clauses of each
  determinate predicate in $\prog'$ has a \cut and $gs', alts'$ and $a'$ are
  list of goals and alternatives build from resp. $gs, alts$ and $a$ where the
  \prog has been by $\prog'$.
  \label{th:all-cut2}
\end{lemma}

\def\clauseL{\ensuremath{\mathcal{L}}\xspace}
\begin{proof}
  Let \clauseL be the list of clauses implementating a determinate predicate. We
  can reason by induction on the length of the elements in \clauseL. If \clauseL
  is empty, then the claim is trivially true. Otherwise, we have a clause $c$
  follwed by a list \clauseL of clauses. If $c$ has a cut then we are done.
  Otherwise, by hypothesis, the head $c$ is mutual exclusive from all other
  clauses in \clauseL. Therefore, adding a cut as first atom of the body of $c$
  will maintain the semantics of the original program: if a goal $g$ unifies
  with the head of $c$, then by hypothesis, no clause $c' \in \clauseL$ has a
  head unifying with $g$. The insertion of the head-cut in the body of $c$
  simply tells the interpreter to discard all clauses in \clauseL that anyway
  could not unify with the goal. If, finally, a goal $g$ does not unify with the
  head of $g$ then adding a head-cut in its body does not affect the behavior
  of the program: the head of the clause fails to be unified with the goal
  and by induction hypothesis, \clauseL is a program where all clauses
  of the determinated predicates have a \cut. This completes the proof.
\end{proof}

Thanks to \cref{th:all-cut}, we can give a common structure to all the clauses
of a determinate predicate: we are free to assume that $c$ always has at
least one cut.

\begin{corollary}
  If all clauses of a predicate have a cut, then at most one of
  these clauses can be applied successfully on the goal, the other being
  cut away.
  $$
  \begin{array}{l}
  \forall \subst\ \subst' i\ o\ p,\\
  \runCmd{[\goalCmd{\mathtt{tailcut}(\prog(p))}{\callCmd{p}{i}{o}}{a}]}{[]}{\subst}{x'}{\subst'} \rightarrow [] = x'
  \end{array}
  $$
  \label{th:only-one-clause}
\end{corollary}

\begin{proof}
  By a slightly modified version of \cite{1989Warren}
\end{proof}

%
\begin{theorem}[determinacy check $\to$ determinate pred]
  Let $\prog$ be a determinacy-checked program, then, using \elpi input/output
  modes, for any determinate predicate $p$, \texttt{\ref{detpred}~\prog~p}
  holds.
  $$
  \begin{array}{l}
  \forall \subst\ \subst' i\ o\ p, \mathtt{det\_check}(p) \to \mathtt{well\_moded}(p,i) \to \\
  \runCmd{[\goalCmd{\prog}{\callCmd{p}{i}{o}}{a}]}{[]}{\subst}{[]}{\subst'} \to
  \end{array}
  $$
\end{theorem}

% Before giving the proof, we just want to point out that using input/output modes
% of \elpi, no hypothesis on the groundness of terms can be done. 

\begin{proof}
  By \cref{th:all-cut} \cref{th:only-one-clause}
\end{proof}



