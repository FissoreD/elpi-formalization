\section{First-order prolog}
\label{sec:basic-elpi}

\subsection{First-order interpreter}
In this first section we start with a first-order version of a logic language.

The interpreter we are going to present is a system based on the derivation
rules depicted in \cref{fig:basic-interp}. Its structure is similar to the
operational semantics in \cite{1990Vink}. The choice to use operational
semantics, rather than denotational semantics (as in \cite{2011king}), is due to
our preference for maintaining a concrete representation of the objects we are
manipulating. Instead of using continuations to store the state of a choice
point, we prefer to represent this information as lists containing the
alternatives, along with the substitution existing at the moment the choice
point was created.

\begin{figure}
  
  \ruleStopM{.45}
  \ruleFailM{.45}
  \vspace{0.3em}%
  
  \ruleUnifM{.50}
  \ruleBangM{.40}
  \vspace{0.3em}%
  
  \ruleCallM{1}
  
  \caption{Basic derivation rules}
  \label{fig:basic-interp}
\end{figure}

The semantics expressed by \runCmd{g}{a}{s}{a'}{s'} relates a list of
goals $g$, a list of alternatives $a$ and an initial substitution $s$ with a
new substitution $s'$ and a new list of alternatives $a'$. The list of goals
should be understood as a list of goal put in cunjunction, whereas the
alternatives represent a disjunction of goals. In the derivation above we use
the two colon symbol ($::$) has separator between the head of a list and its tail;
$@$ stands for list concatenation; $[\ ]$ is the empty list. $\prog p$ is the
application of \prog to the predicate $p$; it returns a list of clauses. The
function $\mathcal{F}$ is defined as follows:
%
$$
\mathcal{F}(\prog, p, i, o, s, a) := 
  [(s, (
      \Cons{(\prog, i =_i i', a)}
        \Cons{(\prog, o =_o o', a)}
          {[(\prog, g, a) \mid g \in bs]})) \mid \clauseCmd{p}{i'\ o'}{bs} \in \prog\ p]
$$

It takes a list of clauses, two terms, i.e. $i$ and $o$ representing the two
arguments of the predicate call a substitution $s$ and a list of alternatives $a$. For each clause
$(cl\ i'\ o'\ bs)$, it builds a new list of pairs where the first argument
is the substitution $s$ and the second are goals made by 1) the unification of
the first argument $i'$ with the first argument $i$ of the predicate call 2) the
unification between the second arguments $o'$ and $o$ 3) the map of each premise
$g$ in the body $bs$ which return a new list of goals having the program $p$,
the atom $g$ and the list of alternatives $a$.

The behaviour of the program depends on the shape of the list of goal and its
alternatives. Starting from a configuration it is possible to execute the
program. It means that it exists a sequence of reductions allowing to reach the
\textit{stop rule}, symbolyzed with \ruleStop.

More precisely, \textit{stop rule} caputres the
configurations with an empty list of goals. By convention, this configuration is
the final one: there are no more goal to treat; therefore, we can stop and
return the same alternatives and substitution received in input.

The \textit{fail rule} (\ruleFail) is used to consume the list of alternatives,
if for example, the current goal leads to a failure or a loop. In fact, this
rule allows to break loop since it can non-determinalistically applied on any
configuration, provided that the list of alternatives is not empty. It is
possible to make the algorithm deterministic by chainging \ruleFail so that it
is applied if the current goal is a call to a predicate with no alternatives,
but we prefer to simplify our rule system.

The \textit{unify rule} (\ruleUnif) is applied when a unification $t1 = t2$
occurs in the head of the current goal list. It calls the unification algorithm
between the two terms $t1$ and $t2$ under the current substitution $s$ and
returns the updated substitution $s'$. Finally the \run procedure is called the
list of remaining goals $gs$ and the new substitution $s'$.

The \textit{cut rule} (\ruleBang) refers to goals where the first conjunct is is
the \texttt{!} atom. This is where the cut-alternatives $ca$ become interesting.
The rule tells that the list of goal $gl$ can be launched using $ca$ and the
new list of alternatives.

The \textit{call rule} (\ruleCall) deals with goals starting with a predicate
call. In this case the function $\mathcal{F}$ is called with the parameters as
explained above and if the result of this operation is the list
\ConsHd{b}\ConsTl{bs}, then a recursive call to \run is done by prepending $b$ to
the list of remaining goals $gl$ and $bs$ is prepended to the list of
alternatives $a$.

The combination of the \ruleCall and \ruleBang are crucial to reproduce the
behaviour of the hard cut. When a call to a predicate gives several rules as new
choice points, thanks to the function $\mathcal{F}$ we can associate each atom
of each body of $\prog p$ to the current alternatives $a$. If a clause $b_i$ in
\ConsHd{b}\ConsTl{bs} with $0 \leq i \leq | bs |$ has a cut, then all the
choice points $b_j$ with $i < j \leq | bs |$ will be cut away.

As an example, let's consider the following program called \prog:

\begin{elpicode}
p1 X Y :- p2 X Y.           % r1
p1 3 3.                     % r2
p2 X Y :- p3 X Y, !, Y = 1. % r3
p2 1 1.                     % r4
p3 1 2.                     % r5
\end{elpicode}

{
\def\goalG{\goalCmd{\prog}{call\ (\text{\elpiIn{p1 3 Z}})}{\EmptyList}}

Let $\mathcal{G} :=\ \goalG$ be a goal, the execution of
``\runCmd{\mathcal{G}}{\EmptyList}{\EmptySubst}{?A}{?\subst}'' will apply \ruleCall
producing a new goal for the rule $r1$ and an alternative list containing $r2$,
we leave out details concerning unifications of head terms. The execution of
$r1$ will try to solve $r3$. This will add three goals \elpiIn{r 1 Y, !, Y = 1}
with cut-alternatives equal to $r2$ whereas the new alternatives will be $r4 @
r2$. The execution of $r3$ will solve \elpiIn{p3 1 Y} with substitution $s :=
\{X \gets 1; Y \gets 2\}$. The \elpiIn{!} will cut away the alternative $r4 @
r2$ and will keep the cut-alternative $r2$. The failing premise \elpiIn{Y = 1}
will cause a backtracking thanks to \ruleFail and try to apply $r2$ from the
empty substitution. This last unification succeed with final substution $?\subst :=
\{X \gets 3; Y \gets 3\}$ and final list of alternatives $?A := \EmptyList$.

% \begin{myRule}{1}
%   \AxiomC{}
%   \RightLabelM{\ruleCall}
%   \UnaryInfC{\runCmd{\mathcal{G}}{\EmptyList}{\EmptySubst}{?A}{?S}}
% \end{myRule}

}


\subsection{Modes in the first-order setting}

% Mode checking in this setting is performed using the classical groundness
% verification where term groundness is derived from their flow in the body of a
% clause. We start with the hypothesis that a term in input position in the head
% of a clause is ground. The premises in the body representing calls to predicates
% must be called with ground input terms, while output are supposed to become
% ground. At the end of the body analyse, we check that the output terms in the
% head of the clause has become ground. If this is not the case, or if a call in
% the body is done with a non-ground term in input position a mode-checking error
% is raised.

The modes we authorize in this setting are two, namely the input and the output
modes. As explained at the very end of \cref{sec:modes}, we want predicate
to be called with non-ground input terms, therefore
the meaning we attach to our modes is quite particuar. This means that
no real
static check is performed on the clauses of the database: in this stage of our
first-order version of the langauge, we give the user high freedom on the shape
that the ipnut of its predicate call may have.

Our interpretation of mode follows the idea of the read and write modes
explained in \cite{1991ait-wam}. Essentially, we consider an input argument as
an object received in read mode in the head of the clause. For example, if the
entry term is a variable and the corresponding term in the head of the clause is
a rigid term, the unification will fail. This is because, the \unify procedure
of the interpreter cannot write in input head-arguments. On the other hand, it
is always possible to unify a non-flex input term with a variable if this
variable appears in the head of the clause. Finally no constraint is put on
output terms, which can be read and written at any time. We will call this
special unification \match which, similarly to the \unify procedure uses the
notation \matchCmd{t_1}{t_2}{\subst}{\subst[']} to signify the \match
between two terms $t_1$ and $t_2$ from an intial substition \subst to a final
substitution \subst['].
Formally, let \coqIn{vars(t)} be a function returning the variables in a 
term \coqIn{t} and \coqIn{is_var(t)} be a function tell if the term \coqIn{t}
is flexible, then:

\begin{coqcode}
  Definition ~\matchCmd{t_1}{t_2}{\subst}{\subst[']}~ :=
    ~\unifyCmd{t_1}{t_2}{\subst}{\subst[']}~ /\ 
      ~$\forall$~ v, v \in vars(~$t_1$~) -> is_var ~(\subst['] $t_1$)~
\end{coqcode}

As a mean of example, consider following program.

\begin{elpicode}
  pred p i:int, o:int.
  p 1 1.          % p1
  p X 2 :- X = 7. % pX
\end{elpicode}


The predicate \elpiIn{p} is decorated with a type\&mode signature. It tells not
only that the first two arguments should be of type \elpiIn{int}, but also that
the first argument is an input (\modeAlone{i}) and the second an output
(\modeAlone{o}). This means that the query \elpiIn{p Y Z} succeed exactly once:
the rule \elpiIn{p1} could not be applied, since the variable \elpiIn{Y} is in
input (i.e. read mode) and cannot be instantiated to $1$ in the head of the
clause. Note, that the application of \elpiIn{pX} on the query succeed and
producing the substitution $\subst = \{Y \mapsto 7, Z \mapsto 2\}$: \elpiIn{Y}
can be instantiated in the body of a clause. Moreover, the query
\elpiIn{p 1 Z} has one solution with $\subst = \{Z \mapsto 1\}$ due to the
application of \elpiIn{p1}. Note that the rule \elpiIn{pX} is a valid choice
point for the query: the head unifies with the query assigning the local
$\exists$variable \elpiIn{X} to $1$, but a failure will occur in the body
while unifying the value of \elpiIn{X} and \elpiIn{7}.

% Moreover, \elpi provides a way to further control the unification on input
% arguments. The user can put the \uvar keyword in front of a variable name in a
% input argument. This tells the unification engine to unify this head term with
% only flexible term. For example, we can add the rule ``\elpiIn{pUX} := \elpiIn{p
% (uvar X) 2 :- X = 7}'' to the previous database. The \uvar keyword does not
% allow the call \elpiIn{p 1 Z} to be unified with \elpiIn{pUX} since \elpiIn{1}
% is not a variable.

% We think that the combination of the input mode and the \uvar keyword give
% the user a powerful tool to  control how unification is dynamically performed
% at runtime.

\subsection{First-order determinacy checker}

Mode checking is an essential ingredient from which determinacy can be
guaranteed. 
% Before explaining the interaction between these two, we need to take
% some time to talk about mutual clauses exclusiveness.
% %
% % \paragraph{Mutual exclusive clauses}
% From \cite{1989Warren}, we know that at most one clause can be applied
% successfully for any determinate predicate.

% In order to satisfy this (necessary by not sufficient) condition, we need 
% that all the clauses of a determinate predicate $p$ are mutually exclusive.
% Mutual exclusiveness can be stated as follows:

% \begin{definition}[Non-overlapping clauses]
%   For any pair of clauses of the same predicate ``\clauseCmd{p}{\vec{x}}{b1}''
%   and ``\clauseCmd{p}{\vec{y}}{b2}'', we say that they are non-overlapping if
%   there exists an input position $i$ such that $\forall \sigma, \sigma\ x_i \neq
%   \sigma\ y_i$
%   \label{def:mut-excl}
% \end{definition}

In \cite{1996henderson}, mode checking (with groundness check)
ensures that at most one clause can be applied to
any predicate. Note the hypotheses called \coqIn{HG} in \cref{def:mut-excl}
and \ref{detpred}. 
In fact, thanks to mode checking,
any call is validated if its terms in input position are ground.
Groundness (modulo \cut, see \cref{def:mut-excl+cut}) ensures that it exists at most one clause 
that can be executed successfully on that call.\\
We want to point out that mode-checking also
ensures that outputs become ground. This is
a fundamental property; otherwise, outputs would be pointless: if an output does
not become ground, it cannot serve as the input for another predicate call.


% In our setting, we slightly extend this definition so that the \elpi's \uvar
% keyword is taken into account.

% \begin{definition}[Mutual exclusiveness with \uvar]
%   A clause with an input term marked with the \elpiIn{uvar} keyword,
%   does not overlap with any other rigid-head term.
%   \label{def:mut-excl-uvar}
% \end{definition}

% This means that a term marked with the \uvar keyword in the head of a clause
% overlaps only with unification variables or with another term marked with
% \uvar.

In our first-order \elpi, we don't perform any static mode analyse. Rather, we
repose on \match operation that is done on the input arguments at runtime.

\begin{definition}[Mutual-exclusion on heads in \elpi]
  \begin{coqcode}
    Definition ~\customlabel{emutexcl}{\texttt{emutual\_exclusive}}\prog \pred\!\!~:
      ~$\forall$~ i, ~$!\exists$~ i' o bo, (~\clauseCmd{\pred}{\texttt{i' o}}{\texttt{bo}}~) \in ~\prog~p -> unify i i'
  \end{coqcode}   
  \label{def:emut-excl} 
\end{definition}

The main difference between this last definition and \cref{def:mut-excl} is the
absence of the \coqIn{HG} hypothesis.

\begin{theorem}
  The \elpi input/ouput modes guarantee that for any
  predicate $p$ whose clauses respect
  \cref{def:emut-excl}, there exists at most
  one succeeding clause for any call to $p$.
\end{theorem}

\begin{proof}
  Without loss of generality, we take a program \prog with only binary
  predicates representing respectively an input and an output. Let $p$ be a
  predicate in \prog such that all clauses respect
  \cref{def:mut-excl}. Let
  ``$p\ t_1\ t_2$'' be a valid call for $p$. Let ``$c_1 :=
  \clauseCmd{p}{t_1'\ t_2'}{b_1}$'' and ``$c_2 := \clauseCmd{p}{t_1''\
  t_2''}{b_2}$'' be two clauses implementing $p$. 
  Note that the absence of
  groundness check avoid us from saying that $t_1$, which is the input of the
  call, is a ground term. We reason by induction on the shape of $t_1$ and show
  that it cannot \match simultaneously with $t_1'$ and $t_1''$, i.e. at most one
  between $c_1$ and $c_2$ can be applied on the goal.
  \begin{itemize}
    \item Case 1: $t_1$ is a constant. A constant, in input position, matches with
          the same constant or a variable. By the definition of \match
          $t_1$ only matches with
          the same constant or a variable.
          Due to \cref{def:mut-excl}, $t_1'$ and $t_1''$ cannot be neither the
          constant $t_1$ nor a unification variable nor a combination of the
          two. Therefore $c_1$ and $c_2$ cannot be applied both of the call to
          $p$.
    \item Case 2: $t_1$ is a variable. A variable, in input position, matches
          only with another variable. 
          By \cref{def:mut-excl}, $t_1'$ and $t_1''$ cannot be both
          unification variables. This means that at most one of the two clauses 
          can be applied on the call to $p$.
    \item Case 3: $t_1$ is a compond term: a term starting with rigid head with
          potentially flexible subterms. If the heads of $t_1'$ and $t_1''$ have
          the same head as $t_1$ then the unification of $t_1$ proceed on the
          subterms, but, by induction hypothesis, only one between $t_1'$ and
          $t_1''$ can unify with $t_1$. If the heads of $t_1'$ and $t_1''$ are
          different then only we are sure that at most one of the two clause
          can be applied on the call.
  \end{itemize}
\end{proof}


% As explained in \cite{1989Warren}, thanks to the (hard-)cut operator, mutual
% exclusiveness can be relaxed.

% \begin{definition}[Mutual exclusiveness with \cut]
%   Two clauses for the same predicate are mutually exclusive if the
%   chronological antecedent has a cut in its body.
%   \label{def:mut-excl-cut}
% \end{definition}

% This ensures that if we reach the cut
% operator in the first clause, the second clause is not considered as a choice
% point. Conversely, if one of the premises before the cut fails, then the second
% clause will be tried. In both situations, the two clauses cannot be applied
% simultaneously to the same predicate call.

% This definition allows overlapping clauses to
% exist in a database under the condition that the antecedent has a cut
% guaranteeing that at most one clause can be applied on a predicate call. 

% \begin{definition}[Mutual-exclusion + \cut]
%   Same as \cref{def:det-prem-cut}
% \end{definition}

% % \paragraph{Determinate clauses after last \cut}
% The second, but no less important, condition for a predicate to be determinate
% is the following:

% \begin{definition}[Determinate premises after last \cut]
%   In each clause of a determinate predicate, the premises after the last \cut
%   operator are only calls to determinate predicates. 
%   \label{def:det-prem-cut}
% \end{definition}

% This guarantees that any output produced is uniquely determined, i.e. no two
% solutions can be produced on the same call.

\begin{definition}[Deterministic\_pred in \elpi]
  \begin{coqcode}
    Definition ~\customlabel{edetpred}{\texttt{edeterminate\_pred}}~(~\prog~: prog) (p: pn) :=
      forall ~$i$~ ~$o$~ ~$a$~
        (H : ~\runCmd{[\goalCmd{\prog}{\callCmd{p}{i}{o}}{\EmptyList}]}{\EmptyList}{\EmptySubst}{a}{\subst}~), ~$a$~ = ~\EmptyList~.
  \end{coqcode}
  \label{def:edt-pred}  
\end{definition}

Our definition of \ref{edetpred} (with a leading \coqIn{e} for
\elpi) changes from \ref{detpred} in \cref{sec:det}: we do not need the \coqIn{HG}
hypothesis: the usage of \elpi modes allows to pass any (even not ground)
term in input position.

\begin{definition}[Determinacy checking in \elpi]
  Determinacy checking (noted \coqIn{edet_check}) on a program
  \prog is equivalent by the combination of 
  \cref{def:emut-excl,def:mut-excl+cut,def:det-prem}
  \label{def:det-check}
\end{definition}

The following lemma says that in a determinacy-checked program, if \pred is
a determinate-annotated program and the \run\ of a call to \pred gives a 
solution, then the same solution is returned by a run of the same goal
in a program where all of the clauses of \pred are rewritten such
that thier last atom is a cut.

\begin{lemma}
  Let \tailcut be a function taking a program \prog and predicate \pred
  returning a new program \prog['] such that the bodies of all clauses of \pred
  in \prog have been added a \cut\ as last atom.

  Let \pred be a determinate-annotated predicate,
  \begin{coqcode}
    Lemma det_tail_cut ~\prog \alt~:
      forall i o a ~\subst \subst[']~ (H: det_check ~\prog\!\!~)
        (HR: ~\runCmd{[\goalCmd{\prog}{\callCmd{\pred}{i}{o}}{\alt}]}{[]}{\subst}{a}{\subst'}~),
          ~\runCmd{[\goalCmd{(\tailcutCmd{\prog}{\pred})}{\callCmd{\pred}{i}{o}}{\alt}]}{[]}{\subst}{a}{\subst'}~.
  \end{coqcode}
  \label{lemma:prog-all-cut}
\end{lemma}

\def\clauseL{\ensuremath{\mathcal{L}}\xspace}
\begin{proof}
  TODO: induction on the derivation in HR
\end{proof}

% Thanks to \cref{th:all-cut}, we can give a common structure to all the clauses
% of a determinate predicate: we are free to assume that $c$ always has at
% least one cut.

The following lemma says that if all clauses of a predicate \pred have a cut as
last atom in their body, if the \run\ of a call to this predicate as a goal with
an empty list of alternatives gives an output, then the alternatives of the
ouput are the empty list.

\begin{lemma}[Tail-cut and cut-alternatives]
  $$
  \begin{array}{l}
  \forall \subst\ \subst' i\ o\ p,\\
  \runCmd{[\goalCmd{(\tailcutCmd{\prog}{\pred})}{\callCmd{p}{i}{o}}{a}]}{[]}{\subst}{x'}{\subst'} \rightarrow [] = x'
  \end{array}
  $$
  \label{lemma:cut-cat-alt}
\end{lemma}

\begin{proof}
  INTUITION: The cut-alternative of the last cut is the empty list
\end{proof}

% \begin{corollary}
%   If all clauses of a predicate have a cut, then at most one of
%   these clauses can be applied successfully on the goal, the other being
%   cut away.
%   \label{cor:only-one-clause}
% \end{corollary}

% \begin{proof}
%   By a slightly modified version of \cite{1989Warren}
% \end{proof}

%
\begin{theorem}[determinacy check $\to$ determinate pred]
  For any determinate-annotated predicate \pred,
  \begin{coqcode}
    Theorem det_check_det_pred ~\prog~ (H: edet_check ~\prog\!\!~):
      ~\ref{edetpred} \prog \pred~.
  \end{coqcode}
\end{theorem}

% Before giving the proof, we just want to point out that using input/output modes
% of \elpi, no hypothesis on the groundness of terms can be done. 

\begin{proof}
  By \cref{lemma:prog-all-cut} and the hypothesis \coqIn{H}, the conclusion can
  be rewritten such that all clauses of \pred have a cut as last atom in their
  bodies. Finally, thanks to \cref{lemma:cut-cat-alt} we can conclude the proof.
\end{proof}



